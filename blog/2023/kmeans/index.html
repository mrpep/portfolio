<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diving deep into K-Means | Leonardo D. Pepino </title> <meta name="author" content="Leonardo D. Pepino"> <meta name="description" content="Let's implement K-Means from scratch in PyTorch"> <meta name="keywords" content="machine-learning, audio, representation-learning, speech"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://leonardopepino.latentsound.com/blog/2023/kmeans/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Diving deep into K-Means",
            "description": "Let's implement K-Means from scratch in PyTorch",
            "published": "December 02, 2023",
            "authors": [
              
              {
                "author": "Leonardo Pepino",
                "authorURL": "https://leonardopepino.latentsound.com",
                "affiliations": [
                  {
                    "name": "University of Buenos Aires",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Leonardo</span> D. Pepino </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Diving deep into K-Means</h1> <p>Let's implement K-Means from scratch in PyTorch</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-is-k-means">What is K-Means?</a> </div> <div> <a href="#why-turning-it-into-a-pytorch-layer">Why turning it into a PyTorch Layer?</a> </div> <div> <a href="#some-ideas-to-try">Some ideas to try</a> </div> <div> <a href="#the-code">The code</a> </div> <div> <a href="#checking-it-works">Checking it works</a> </div> <div> <a href="#k-means">K-Means++</a> </div> <div> <a href="#victory">Victory?</a> </div> <div> <a href="#within-class-variance-and-reinitialization">Within-Class Variance and Reinitialization</a> </div> </nav> </d-contents> <h3 id="what-is-k-means">What is K-Means?</h3> <p>K-Means is an algorithm that partitions a dataset into K distinct non-overlapping clusters. Every data point is assigned to one and only one cluster. This partitioning is useful for clustering and also for quantization, as we can describe a data point with the index corresponding to the assigned cluster. The algorithm is very simple and aims to minimize the within-cluster variance:</p> <p>1) Randomly initialize K vectors (centroids) with the same dimensionality as the data. 2) Assign each data point to its closest centroid. 3) Update each centroid as the mean of all data points assigned to it. 4) Iterate 2 and 3 until centroids stop changing.</p> <p>K-Means doesn’t guarantee convergence to a global minimum, hence it is sensible to the initialization. Some smarter than random initializations have been proposed like K-Means++<d-cite key="arthur2007k"></d-cite> and also running the algorithm with different initializations several times and choosing the solution with lowest within-cluster variance.</p> <p>Another drawback of K-Means is that it doesn’t work well in datasets with non-globular shapes. This is because of euclidean distance, ie. in the moons dataset, the points at the extreme of a moon are further away (in euclidean distance) than the extreme of a moon and the center of the other one.</p> <h3 id="why-turning-it-into-a-pytorch-layer">Why turning it into a PyTorch Layer?</h3> <p>In the context of deep learning, K-Means can be quite handy to discretize data points. For example, in HuBERT<d-cite key="hsu2021hubert"></d-cite> discrete targets are created first from audio vectors (MFCCs) and then from internal layers by applying K-Means. For representation learning, it can be useful to promote the formation of clusters<d-cite key="fard2020deep"></d-cite>. It would be nice to have a K-Means pytorch layer with the following properties:</p> <ul> <li>It can be plugged into any neural network and perform k-means during the training.</li> <li>It can adapt to changing data (online). This is important if we are clustering internal representations from a neural network as they will change during training.</li> <li>We don’t want centroids to collapse or be inactive.</li> </ul> <h3 id="some-ideas-to-try">Some ideas to try</h3> <ul> <li>Our dataset will change during training, either because we are batching or because the distribution is shifting as the neural network learns. If the centroids change too fast adapting completely to the new dataset, it is likely that they will move too much, and if these centroids are our targets, this can be harmful. One idea is to perform an exponential moving average. This way, the centroids perform gradual updates, moving towards the new locations.</li> <li>Another idea, to alleviate the collapse/initialization problem, is to monitor how much the different centroids are being used. If a centroid is inactive, we can replace it. With what? It could be a random point from the dataset, or maybe we can choose the data point that is furthest from any centroid. This idea of choosing a point far from all centroids is also applied in K-Means++ where data points have a higher probability of being chosen as a centroid if they are far from any existing centroid. This way, we are minimizing the risk of duplicating a centroid, and we are maximizing the coverage of the data space.</li> </ul> <p>Some of these ideas are used in random vector quantization<d-cite key="zeghidour2021soundstream"></d-cite><d-cite key="defossez2022highfi"></d-cite> to learn codebooks efficiently.</p> <h3 id="the-code">The code</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
</pre></td> <td class="code"><pre><span class="k">class</span> <span class="nc">K</span><span class="o">-</span><span class="nc">MeansOnlineLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">ema_decay</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">expire_threshold</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">replacement_strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">furthest</span><span class="sh">'</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span> <span class="o">=</span> <span class="n">ema_decay</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expire_threshold</span> <span class="o">=</span> <span class="n">expire_threshold</span>
        <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">=</span> <span class="n">replacement_strategy</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_init_parameters</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">_init_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">codebook</span><span class="sh">'</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">cluster_size</span><span class="sh">'</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,)))</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">codebook_sum</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">y_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">((</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">y_norm</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">xy</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">distances</span>

    <span class="k">def</span> <span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1">#Calculate euclidean distances against all centroids.
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">)</span>
        <span class="c1">#Find the closest centroids for each data point.
</span>        <span class="n">closest_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span>

    <span class="k">def</span> <span class="nf">update_centroids</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">closest_indices</span><span class="p">):</span>
        <span class="c1"># Create a mask for each cluster
</span>        <span class="n">closest_indices_expanded</span> <span class="o">=</span> <span class="n">closest_indices</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">closest_indices_expanded</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">device</span><span class="p">))</span>

        <span class="c1"># For each centroid sum all the points:
</span>        <span class="n">cluster_sums</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">data</span><span class="p">)</span>
        <span class="c1"># Also count how many points there are in each centroid:
</span>        <span class="n">cluster_counts</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="c1"># Update the cluster sizes and codebook sums using EMA:
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">cluster_counts</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">cluster_sums</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span><span class="p">)</span>
        <span class="c1"># Update the centroids by dividing the sum with the size (mean)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">codebook</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">expire_centroids</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">distances</span><span class="p">):</span>
        <span class="c1"># Find unused centroids:
</span>        <span class="n">idxs_replace</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">expire_threshold</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">idxs_replace</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1">#Replace the unused centroids:
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">furthest</span><span class="sh">'</span><span class="p">:</span>
                <span class="n">replace_points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span><span class="p">)[</span><span class="o">-</span><span class="n">num</span><span class="p">:]</span>
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="p">:</span>
                <span class="n">replace_points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randperm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)[:</span><span class="n">num</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">replace_points</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">replace_points</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">update_centroids</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closest_indices</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">expire_centroids</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">distances</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">closest_indices</span>
</pre></td> </tr></tbody></table></code></pre></figure> <h3 id="checking-it-works">Checking it works</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blob-static.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>It seems to be working fine, but… each batch will be a different set of points following a similar distribution to the previous batch. Let’s simulate that situation by sampling at each step a different dataset following the same distribution:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blobsv.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>Keeps working! The centroids don’t update too fast thanks to the exponential moving average. Let’s deactivate the EMA and see what happens:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-noema.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>It still works but the centroids change a lot faster, which could get us in trouble? if the assigned cluster is the target of a neural network.</p> <p>Not only each batch is a different sample of the dataset distribution, but if the dataset comes from an internal representation of a neural network, or if the distribution shifts because data is dynamic (culture changes, different biases are introduced during data collection, etc…), then the distributions of the batches will change. Let’s check the robustness of our model:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blobs-online-fail.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>It seems to be keeping track of the distribution shifts as the centroids follow closely each gaussian mean, but we have a problem: initialization is leading us to a not so good solution. At the beginning of the animation you can see that 2 of the 3 centroids are close each other. This causes them to distribute the points of one cluster between them. The remaining cluster gets the points belonging to the 2 other gaussians. Ideally we want a centroid for each gaussian but we are getting 2 for one, and one for 2. Can we solve it? Well in theory no, because we can’t guarantee convergence to a global minimum in polynomial time. But we can implement the K-Means++ heuristic in our layer to have a better initialization.</p> <h3 id="k-means">K-Means++</h3> <p>The algorithm for K-Means++ initialization is quite simple:</p> <p>1) Pick a random data point as the first centroid. 2) Calculate the distances $D(x)$ from each point to the closest centroid. 3) Build a probability distribution so that the higher distance, the higher probability: $P(x’) = \frac{D^2(x’)}{\sum_{x \in X} D^2(x)}$ 4) Sample a data point following $P(x)$ and add it as a new centroid. 5) Repeat steps 2-4 until there are K centroids.</p> <p>The implementation in PyTorch is quite straight-forward, adding this method to our K-Means layer:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="code"><pre><span class="k">def</span> <span class="nf">K</span><span class="o">-</span><span class="nc">Means_init</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">))</span>
    <span class="c1">#Initial centroid is a randomly sampled point
</span>    <span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="c1">#Distance between each point and closest centroid (only one we have):
</span>    <span class="n">min_distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1">#Turn distances into probabilities:
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">min_distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">min_distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1">#Sample following the probs:
</span>        <span class="n">centroid_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="n">replacement</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1">#Add the new sampled centroid:
</span>        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">centroid_idx</span><span class="p">]</span>
        <span class="c1">#Update the distances:
</span>        <span class="n">distances_new_centroid</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">min_distances</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">minimum</span><span class="p">(</span><span class="n">min_distances</span><span class="p">,</span> <span class="n">distances_new_centroid</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">codebook</span> <span class="o">=</span> <span class="n">centroids</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Check out our previous initialization vs K-Means++ one:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/kmeans/kmeans-init-480.webp 480w,/assets/img/posts/kmeans/kmeans-init-800.webp 800w,/assets/img/posts/kmeans/kmeans-init-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/kmeans/kmeans-init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Random initialization vs K-Means++ initialization. </div> <p>A lot better!</p> <h3 id="victory">Victory?</h3> <p>Not quite yet! Let’s see what happens if we run the same algorithm many times:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/kmeans/kmeans-seeds-480.webp 480w,/assets/img/posts/kmeans/kmeans-seeds-800.webp 800w,/assets/img/posts/kmeans/kmeans-seeds-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/kmeans/kmeans-seeds.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Different runs of K-Means over the same dataset. The red diamonds are the initial centroids determined by K-Means++, the red crosses are the centroids after 50 iterations. </div> <p>We found the ‘good solution’ 9 times out of 10. Can you spot the bad one?… It’s second row, right column. Well, as I said before, K-Means doesn’t guarantee finding the global minimum. Also, K-Means++ is still a random initialization, so we can sample 2 centroids that are very close each other (this is what happened in the bad solution). What can we do?</p> <p>Well, let’s just run the algorithm a few times and then choose the best clustering. How can we measure a good clustering?</p> <h3 id="within-class-variance-and-reinitialization">Within-Class Variance and Reinitialization</h3> <p>K-Means tries to minimize the within-cluster variance (WCV), so let’s run K-Means++ 10 times and measure the resulting WCV. Then, let’s choose the initialization with minimum WCV. The WCV is defined as: \(WCV(X,C) = \sum_{k=1}^K \frac{1}{|C_k|}\sum_{i,i' \in C_k} \sum_{j=1}^D (x_{ij} - x_{i'j})^2\)</p> <p>where $X$ is our dataset with dimensionality $D$ and $C$ is the cluster assignment so that $C_k$ is the set of indexs corresponding to the cluster $k$. The idea is that we are calculating the pairwise euclidean distance for all the points belonging to a cluster. If the points in each cluster are close each other, then the WCV will be low.</p> <p>The code for calculating WCV is:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td> <td class="code"><pre><span class="k">def</span> <span class="nf">wcv</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">cvsum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">):</span>
        <span class="n">ck</span> <span class="o">=</span> <span class="n">closest_indices</span> <span class="o">==</span> <span class="n">i</span>
        <span class="n">ck_cardinal</span> <span class="o">=</span> <span class="n">ck</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ck_cardinal</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">ck</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">ck</span><span class="p">]))</span><span class="o">/</span><span class="n">ck_cardinal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cv</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cvsum</span> <span class="o">+=</span> <span class="n">cv</span>
    <span class="k">return</span> <span class="n">cvsum</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>The WCV values at initialization and after 50 iterations can be seen in the titles of the previous figure. While the good solutions end up with a WCV of 3781, the bad one ends up with 9683. This hints us that running multiple seeds and choosing the one with less WCV is a good approach to avoid bad solutions. Also notice that WCV always decreases with K-Means algorithm.</p> <p>You can find the final code and experiments of this article in <a href="https://colab.research.google.com/drive/1VSMV87z7jp3JwfuSMGl2BSussvoraTGQ?usp=sharing" rel="external nofollow noopener" target="_blank">this colab</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-12-02-kmeans.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Leonardo D. Pepino. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>