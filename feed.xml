<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://leonardopepino.latentsound.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leonardopepino.latentsound.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-18T05:11:13+00:00</updated><id>https://leonardopepino.latentsound.com/feed.xml</id><title type="html">blank</title><subtitle>Teaching machines to listen (and speak) </subtitle><entry><title type="html">Your own overleaf</title><link href="https://leonardopepino.latentsound.com/blog/2024/overleaf/" rel="alternate" type="text/html" title="Your own overleaf"/><published>2024-11-24T14:14:00+00:00</published><updated>2024-11-24T14:14:00+00:00</updated><id>https://leonardopepino.latentsound.com/blog/2024/overleaf</id><content type="html" xml:base="https://leonardopepino.latentsound.com/blog/2024/overleaf/"><![CDATA[<p><a href="https://www.overleaf.com/">Overleaf</a> is a collaborative cloud-based latex editor. Itâ€™s very popular among researchers and students as it allows to collaborate in document writing using Latex, and to compile these documents without having to install packages. However, your documents are in somebody else server, and you have to pay a subscription to collaborate with more than one person.</p> <p>In this article, Iâ€™m going to show you how I overcame these problems by hosting my own version of overleaf. This way, I donâ€™t have to pay subscriptions, and all the documents stay in my computer.</p> <h3 id="running-the-official-overleaf">Running the official overleaf</h3> <p>Apart from offering the cloud service, Overleaf has released a <a href="https://github.com/overleaf/overleaf">free community edition</a> that can be installed locally.</p> <p>Installation is very easy and well documented.</p> <p>1) Clone the Overleaf toolkit repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/overleaf/toolkit.git ./overleaf-toolkit
</code></pre></div></div> <p>2) Install docker following their <a href="https://docs.docker.com/engine/install/ubuntu/">instructions</a> 3) Move to the repository:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ./overleaf-toolkit
</code></pre></div></div> <p>4) And generate the config files:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bin/init
</code></pre></div></div> <p>After running the command youâ€™ll find 3 files in the config folder, which can be edited to customize the Overleaf instance. Some things you might want to tweak are:</p> <p>In overleaf.rc</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OVERLEAF_DATA_PATH=[where you want documents to be stored]
OVERLEAF_PORT=[port for overleaf]
</code></pre></div></div> <p>In variables.env</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OVERLEAF_NAV_TITLE=[your cool title to display]
OVERLEAF_ADMIN_EMAIL=[who to contact when things stop working]
OVERLEAF_EMAIL_SMTP_... [these variables are to setup the email server]
OVERLEAF_HEADER_IMAGE_URL=[your cool logo]
</code></pre></div></div> <p>5) Launch it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bin/up
</code></pre></div></div> <p>6) Go to https://127.0.0.1:[OVERLEAF_PORT]/launchpad and create your user.</p> <p>Now you can start using your own Overleaf!</p> <h3 id="adding-packages">Adding packages</h3> <p>We can install all the available TexLive packages, adding between 3 and 4 GB to the container.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec </span>sharelatex tlmgr <span class="nb">install </span>scheme-full
docker <span class="nb">exec </span>sharelatex tlmgr path add
</code></pre></div></div> <p>After that we can commit the changes and save the updated container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker commit sharelatex <span class="nb">local</span>/sharelatex-with-texlive-full:5.2.1
</code></pre></div></div> <p>Make sure that the overleaf version, found in config/version, matches the container name (5.2.1 in my case). Then stop the overleaf container (by running Ctrl+C), and edit the config/overleaf.rc file updating the SHARELATEX_IMAGE_NAME variable:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">SHARELATEX_IMAGE_NAME</span><span class="o">=</span><span class="nb">local</span>/sharelatex-with-texlive-full 
</code></pre></div></div> <p>Launch again the overleaf container and check that it works.</p> <h3 id="adding-comments-and-tracking-changes">Adding comments and tracking changes</h3> <p>By default the overleaf community edition image doesnâ€™t have the comment feature (<span style="color:red">Now it does! although with this method you can add review/track changes functions ;)</span>.). Luckily, some smart people made an <a href="https://github.com/yu-i-i/overleaf-cep/tree/ldap-tc">extended overleaf</a> tackling this issue. We can follow the instructions in this <a href="https://github.com/overleaf/overleaf/issues/1193#issuecomment-2256681075">issue</a> to enable comments.</p> <p>Run the following command to open an interactive bash terminal in the overleaf container:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> sharelatex bash
</code></pre></div></div> <p>Then make sure you are in the overleaf folder and execute these commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/yu-i-i/overleaf-cep.git overleaf-cep
<span class="nb">mv </span>overleaf-cep/services/web/modules/track-changes services/web/modules/track-changes
<span class="nb">rm</span> <span class="nt">-rf</span> overleaf-cep
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s2">"/moduleImportSequence:/a 'track-changes',"</span> services/web/config/settings.defaults.js
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/trackChangesAvailable: false/trackChangesAvailable: true/g'</span> services/web/app/src/Features/Project/ProjectEditorHandler.js
</code></pre></div></div> <p>Exit the interactive terminal and then commit the changes to the image:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker commit sharelatex <span class="nb">local</span>/sharelatex-with-texlive-full-tc:5.2.1
</code></pre></div></div> <p>Finally, update the SHARELATEX_IMAGE_NAME variable in the config/overleaf.rc file.</p> <h3 id="access-from-the-outside-world">Access from the outside world</h3> <p>If you are happy with how overleaf works locally, you might want to access it from any network, and add users, etcâ€¦ There are many ways to do that, but I find it very simple to use CloudFlare Tunnel if you own a domain, or ngrok if you donâ€™t.</p> <p>Ngrok is very easy to setup:</p> <p>1) Go to https://ngrok.com/ 2) Sign up or Login 3) Follow the instructions to run ngrok in your computer and setup the auth token. 4) You get a free static domain to use. It will be shown in your dashboard. Run the command shown and change the port to the overleaf one:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ngrok http <span class="nt">--url</span><span class="o">=[</span>YOUR_STATIC_DOMAIN] <span class="o">[</span>OVERLEAF_PORT]
</code></pre></div></div> <p>5) Now go to your domain url and enjoy your free overleaf ðŸ˜ƒ</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/overleaf/login-page-480.webp 480w,/assets/img/posts/overleaf/login-page-800.webp 800w,/assets/img/posts/overleaf/login-page-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/overleaf/login-page.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="sync-with-git">Sync with git</h3> <p>Finally, if your server dies, you donâ€™t want to lose all those precious latex files. You can regularly backup the overleaf folders, which is fine, but it kind of forces you to reinstall the overleaf container again and hope everything still works.</p> <p>Another approach is to have a github repository where the tex files are backuped. For that purpose I created this <a href="https://github.com/mrpep/overleaf-git-sync">script</a>, which is very easy to use. You can check the readme with instructions, but basically all is needed is to create a repository, and then modify the config.json file adding details about the domain you used in the previous step, username, password, and the local path to the repository you created for backup.</p> <h3 id="associate-an-email">Associate an email</h3> <p>You might want to invite new users or let them reset their passwords. The manual way to do this is to go to Admin/Manage Users and register the user email. This will generate an invitation link that you can share with the person. Remember to replace localhost with your overleaf domain from Ngrok.</p> <p>However this can be tedious as you have to manually do this for every new user. A better approach is to associate an email for administration purposes. I will explain the process for gmail accounts:</p> <p>1) Generate an app password. This is needed as gmail will ask for 2 factor authentication, but we need a password that overleaf can use to access our account. Go <a href="https://myaccount.google.com/apppasswords">here</a>, enter an app name, ie. Overleaf, and then copy the generated password. 2) Modify these settings in the variables.env file:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OVERLEAF_EMAIL_FROM_ADDRESS=your-username@gmail.com
OVERLEAF_EMAIL_SMTP_HOST=smtp.gmail.com
OVERLEAF_EMAIL_SMTP_PORT=587
OVERLEAF_EMAIL_SMTP_SECURE=false
OVERLEAF_EMAIL_SMTP_USER=your-username@gmail.com
OVERLEAF_EMAIL_SMTP_PASS=the-pass-you-created
</code></pre></div></div> <p>3) Run bin/up again to refresh the instance and we are ready to go!</p>]]></content><author><name></name></author><category term="linux"/><category term="overleaf"/><category term="academics"/><category term="phd-life"/><summary type="html"><![CDATA[A short guide to deploy your own overleaf and own your data with pro features for free.]]></summary></entry><entry><title type="html">Diving deep into K-Means</title><link href="https://leonardopepino.latentsound.com/blog/2023/kmeans/" rel="alternate" type="text/html" title="Diving deep into K-Means"/><published>2023-12-02T00:00:00+00:00</published><updated>2023-12-02T00:00:00+00:00</updated><id>https://leonardopepino.latentsound.com/blog/2023/kmeans</id><content type="html" xml:base="https://leonardopepino.latentsound.com/blog/2023/kmeans/"><![CDATA[<h3 id="what-is-k-means">What is K-Means?</h3> <p>K-Means is an algorithm that partitions a dataset into K distinct non-overlapping clusters. Every data point is assigned to one and only one cluster. This partitioning is useful for clustering and also for quantization, as we can describe a data point with the index corresponding to the assigned cluster. The algorithm is very simple and aims to minimize the within-cluster variance:</p> <p>1) Randomly initialize K vectors (centroids) with the same dimensionality as the data. 2) Assign each data point to its closest centroid. 3) Update each centroid as the mean of all data points assigned to it. 4) Iterate 2 and 3 until centroids stop changing.</p> <p>K-Means doesnâ€™t guarantee convergence to a global minimum, hence it is sensible to the initialization. Some smarter than random initializations have been proposed like K-Means++<d-cite key="arthur2007k"></d-cite> and also running the algorithm with different initializations several times and choosing the solution with lowest within-cluster variance.</p> <p>Another drawback of K-Means is that it doesnâ€™t work well in datasets with non-globular shapes. This is because of euclidean distance, ie. in the moons dataset, the points at the extreme of a moon are further away (in euclidean distance) than the extreme of a moon and the center of the other one.</p> <h3 id="why-turning-it-into-a-pytorch-layer">Why turning it into a PyTorch Layer?</h3> <p>In the context of deep learning, K-Means can be quite handy to discretize data points. For example, in HuBERT<d-cite key="hsu2021hubert"></d-cite> discrete targets are created first from audio vectors (MFCCs) and then from internal layers by applying K-Means. For representation learning, it can be useful to promote the formation of clusters<d-cite key="fard2020deep"></d-cite>. It would be nice to have a K-Means pytorch layer with the following properties:</p> <ul> <li>It can be plugged into any neural network and perform k-means during the training.</li> <li>It can adapt to changing data (online). This is important if we are clustering internal representations from a neural network as they will change during training.</li> <li>We donâ€™t want centroids to collapse or be inactive.</li> </ul> <h3 id="some-ideas-to-try">Some ideas to try</h3> <ul> <li>Our dataset will change during training, either because we are batching or because the distribution is shifting as the neural network learns. If the centroids change too fast adapting completely to the new dataset, it is likely that they will move too much, and if these centroids are our targets, this can be harmful. One idea is to perform an exponential moving average. This way, the centroids perform gradual updates, moving towards the new locations.</li> <li>Another idea, to alleviate the collapse/initialization problem, is to monitor how much the different centroids are being used. If a centroid is inactive, we can replace it. With what? It could be a random point from the dataset, or maybe we can choose the data point that is furthest from any centroid. This idea of choosing a point far from all centroids is also applied in K-Means++ where data points have a higher probability of being chosen as a centroid if they are far from any existing centroid. This way, we are minimizing the risk of duplicating a centroid, and we are maximizing the coverage of the data space.</li> </ul> <p>Some of these ideas are used in random vector quantization<d-cite key="zeghidour2021soundstream"></d-cite><d-cite key="defossez2022highfi"></d-cite> to learn codebooks efficiently.</p> <h3 id="the-code">The code</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">K</span><span class="o">-</span><span class="nc">MeansOnlineLayer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">ema_decay</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">expire_threshold</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">replacement_strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">furthest</span><span class="sh">'</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span> <span class="o">=</span> <span class="n">ema_decay</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expire_threshold</span> <span class="o">=</span> <span class="n">expire_threshold</span>
        <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">=</span> <span class="n">replacement_strategy</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_init_parameters</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">_init_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">codebook</span><span class="sh">'</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">cluster_size</span><span class="sh">'</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,)))</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">codebook_sum</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">y_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tile</span><span class="p">((</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">y_norm</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">xy</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">distances</span>

    <span class="k">def</span> <span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="c1">#Calculate euclidean distances against all centroids.
</span>        <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">)</span>
        <span class="c1">#Find the closest centroids for each data point.
</span>        <span class="n">closest_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span>

    <span class="k">def</span> <span class="nf">update_centroids</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">closest_indices</span><span class="p">):</span>
        <span class="c1"># Create a mask for each cluster
</span>        <span class="n">closest_indices_expanded</span> <span class="o">=</span> <span class="n">closest_indices</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">closest_indices_expanded</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">device</span><span class="p">))</span>

        <span class="c1"># For each centroid sum all the points:
</span>        <span class="n">cluster_sums</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">data</span><span class="p">)</span>
        <span class="c1"># Also count how many points there are in each centroid:
</span>        <span class="n">cluster_counts</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="c1"># Update the cluster sizes and codebook sums using EMA:
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">cluster_counts</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span><span class="p">.</span><span class="nf">lerp_</span><span class="p">(</span><span class="n">cluster_sums</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">ema_decay</span><span class="p">)</span>
        <span class="c1"># Update the centroids by dividing the sum with the size (mean)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">codebook</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">expire_centroids</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">distances</span><span class="p">):</span>
        <span class="c1"># Find unused centroids:
</span>        <span class="n">idxs_replace</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">expire_threshold</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">idxs_replace</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1">#Replace the unused centroids:
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">furthest</span><span class="sh">'</span><span class="p">:</span>
                <span class="n">replace_points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span><span class="p">)[</span><span class="o">-</span><span class="n">num</span><span class="p">:]</span>
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">replacement_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="p">:</span>
                <span class="n">replace_points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randperm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)[:</span><span class="n">num</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">codebook</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">replace_points</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">cluster_size</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="n">self</span><span class="p">.</span><span class="n">codebook_sum</span><span class="p">[</span><span class="n">idxs_replace</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">replace_points</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">update_centroids</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closest_indices</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">expire_centroids</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">distances</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">closest_indices</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="checking-it-works">Checking it works</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blob-static.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>It seems to be working fine, butâ€¦ each batch will be a different set of points following a similar distribution to the previous batch. Letâ€™s simulate that situation by sampling at each step a different dataset following the same distribution:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blobsv.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>Keeps working! The centroids donâ€™t update too fast thanks to the exponential moving average. Letâ€™s deactivate the EMA and see what happens:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-noema.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>It still works but the centroids change a lot faster, which could get us in trouble? if the assigned cluster is the target of a neural network.</p> <p>Not only each batch is a different sample of the dataset distribution, but if the dataset comes from an internal representation of a neural network, or if the distribution shifts because data is dynamic (culture changes, different biases are introduced during data collection, etcâ€¦), then the distributions of the batches will change. Letâ€™s check the robustness of our model:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/posts/kmeans/kmeans-blobs-online-fail.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>It seems to be keeping track of the distribution shifts as the centroids follow closely each gaussian mean, but we have a problem: initialization is leading us to a not so good solution. At the beginning of the animation you can see that 2 of the 3 centroids are close each other. This causes them to distribute the points of one cluster between them. The remaining cluster gets the points belonging to the 2 other gaussians. Ideally we want a centroid for each gaussian but we are getting 2 for one, and one for 2. Can we solve it? Well in theory no, because we canâ€™t guarantee convergence to a global minimum in polynomial time. But we can implement the K-Means++ heuristic in our layer to have a better initialization.</p> <h3 id="k-means">K-Means++</h3> <p>The algorithm for K-Means++ initialization is quite simple:</p> <p>1) Pick a random data point as the first centroid. 2) Calculate the distances $D(x)$ from each point to the closest centroid. 3) Build a probability distribution so that the higher distance, the higher probability: $P(xâ€™) = \frac{D^2(xâ€™)}{\sum_{x \in X} D^2(x)}$ 4) Sample a data point following $P(x)$ and add it as a new centroid. 5) Repeat steps 2-4 until there are K centroids.</p> <p>The implementation in PyTorch is quite straight-forward, adding this method to our K-Means layer:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">K</span><span class="o">-</span><span class="nc">Means_init</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">centroids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">))</span>
    <span class="c1">#Initial centroid is a randomly sampled point
</span>    <span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">batch_size</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="c1">#Distance between each point and closest centroid (only one we have):
</span>    <span class="n">min_distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1">#Turn distances into probabilities:
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">min_distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">min_distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1">#Sample following the probs:
</span>        <span class="n">centroid_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="n">replacement</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1">#Add the new sampled centroid:
</span>        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">centroid_idx</span><span class="p">]</span>
        <span class="c1">#Update the distances:
</span>        <span class="n">distances_new_centroid</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">min_distances</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">minimum</span><span class="p">(</span><span class="n">min_distances</span><span class="p">,</span> <span class="n">distances_new_centroid</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">codebook</span> <span class="o">=</span> <span class="n">centroids</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Check out our previous initialization vs K-Means++ one:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/kmeans/kmeans-init-480.webp 480w,/assets/img/posts/kmeans/kmeans-init-800.webp 800w,/assets/img/posts/kmeans/kmeans-init-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/kmeans/kmeans-init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Random initialization vs K-Means++ initialization. </div> <p>A lot better!</p> <h3 id="victory">Victory?</h3> <p>Not quite yet! Letâ€™s see what happens if we run the same algorithm many times:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/kmeans/kmeans-seeds-480.webp 480w,/assets/img/posts/kmeans/kmeans-seeds-800.webp 800w,/assets/img/posts/kmeans/kmeans-seeds-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/kmeans/kmeans-seeds.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Different runs of K-Means over the same dataset. The red diamonds are the initial centroids determined by K-Means++, the red crosses are the centroids after 50 iterations. </div> <p>We found the â€˜good solutionâ€™ 9 times out of 10. Can you spot the bad one?â€¦ Itâ€™s second row, right column. Well, as I said before, K-Means doesnâ€™t guarantee finding the global minimum. Also, K-Means++ is still a random initialization, so we can sample 2 centroids that are very close each other (this is what happened in the bad solution). What can we do?</p> <p>Well, letâ€™s just run the algorithm a few times and then choose the best clustering. How can we measure a good clustering?</p> <h3 id="within-class-variance-and-reinitialization">Within-Class Variance and Reinitialization</h3> <p>K-Means tries to minimize the within-cluster variance (WCV), so letâ€™s run K-Means++ 10 times and measure the resulting WCV. Then, letâ€™s choose the initialization with minimum WCV. The WCV is defined as: \(WCV(X,C) = \sum_{k=1}^K \frac{1}{|C_k|}\sum_{i,i' \in C_k} \sum_{j=1}^D (x_{ij} - x_{i'j})^2\)</p> <p>where $X$ is our dataset with dimensionality $D$ and $C$ is the cluster assignment so that $C_k$ is the set of indexs corresponding to the cluster $k$. The idea is that we are calculating the pairwise euclidean distance for all the points belonging to a cluster. If the points in each cluster are close each other, then the WCV will be low.</p> <p>The code for calculating WCV is:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">wcv</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">closest_indices</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">closest_codebook_entry</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">cvsum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">):</span>
        <span class="n">ck</span> <span class="o">=</span> <span class="n">closest_indices</span> <span class="o">==</span> <span class="n">i</span>
        <span class="n">ck_cardinal</span> <span class="o">=</span> <span class="n">ck</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ck_cardinal</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">ck</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">ck</span><span class="p">]))</span><span class="o">/</span><span class="n">ck_cardinal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cv</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cvsum</span> <span class="o">+=</span> <span class="n">cv</span>
    <span class="k">return</span> <span class="n">cvsum</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The WCV values at initialization and after 50 iterations can be seen in the titles of the previous figure. While the good solutions end up with a WCV of 3781, the bad one ends up with 9683. This hints us that running multiple seeds and choosing the one with less WCV is a good approach to avoid bad solutions. Also notice that WCV always decreases with K-Means algorithm.</p> <p>You can find the final code and experiments of this article in <a href="https://colab.research.google.com/drive/1VSMV87z7jp3JwfuSMGl2BSussvoraTGQ?usp=sharing">this colab</a></p>]]></content><author><name>Leonardo Pepino</name></author><category term="machine-learning"/><category term="pytorch"/><category term="clustering"/><category term="k-means"/><summary type="html"><![CDATA[Let's implement K-Means from scratch in PyTorch]]></summary></entry></feed>